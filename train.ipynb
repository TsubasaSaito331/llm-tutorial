{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ç°¡å˜ãªQ&Aã‚’è¡Œã†LLMã‚’ä½œæˆã™ã‚‹\n",
    "ã“ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã§ã¯ã€Hugging Faceã®Transformersã¨PEFTã‚’ä½¿ç”¨ã—ã¦ã€ç°¡å˜ãªQ&Aã‚’è¡Œã†LLMã‚’ä½œæˆã—ã¾ã™ã€‚  \n",
    "ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¯hugging faceã§å…¬é–‹ã•ã‚Œã¦ã„ã‚‹datasetã‚’ä½¿ç”¨ã—ã¦Instruction Tuningã‚’è¡Œã„ã¾ã™ã€‚  \n",
    "ä»Šå›ä½¿ç”¨ã™ã‚‹LLMã¯è»½é‡ãª\"llm-jp/llm-jp-3-1.8b\"ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚  \n",
    "\n",
    "ãƒªãƒ³ã‚¯ï¼š  \n",
    "- ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆï¼šhttps://huggingface.co/datasets/izumi-lab/llm-japanese-dataset\n",
    "- ãƒ¢ãƒ‡ãƒ«ï¼šhttps://huggingface.co/llm-jp/llm-jp-3-1.8b\n",
    "-  å‚è€ƒï¼šhttps://qiita.com/m__k/items/173ade78990b7d6a4be4\n",
    "\n",
    "ãƒãƒ¼ã‚¸ãƒ§ãƒ³æƒ…å ±ï¼š  \n",
    "- python : 3.12.4\n",
    "- cuda : 12.1\n",
    "- transformers : 4.44.2\n",
    "- torch : 2.3.1+cu121\n",
    "- peft : 0.11.1\n",
    "- accelerate : 0.32.1\n",
    "- datasets : 2.20.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### ãƒ¢ãƒ‡ãƒ«ã¨ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ã®èª­ã¿è¾¼ã¿\n",
    "config.yamlã‚’èª­ã¿è¾¼ã‚“ã§æŒ‡å®šã—ãŸãƒ¢ãƒ‡ãƒ«ã¨ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¾ã™ã€‚  \n",
    "åˆå›å®Ÿè¡Œæ™‚ã¯ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã«æ™‚é–“ãŒã‹ã‹ã‚‹ã“ã¨ãŒã‚ã‚Šã¾ã™ã€‚  \n",
    "ãƒ¢ãƒ‡ãƒ«ã¯ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã¨ã—ã¦ä¿å­˜ã•ã‚Œã‚‹ãŸã‚ã€å¿…è¦ã®ãªã„ãƒ¢ãƒ‡ãƒ«ã¯å‰Šé™¤ã—ã¦ãã ã•ã„ã€‚  \n",
    "ãƒ¢ãƒ‡ãƒ«ã®ä¿å­˜å…ˆï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼‰ï¼š\n",
    "~/.cache/huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import yaml\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TextStreamer\n",
    "\n",
    "# YAMLãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿\n",
    "with open(\"./config.yaml\", \"r\") as file:\n",
    "    config = yaml.safe_load(file)\n",
    "\n",
    "# ãƒ¢ãƒ‡ãƒ«ã®è¨­å®š\n",
    "model_config = config[\"model_config\"]\n",
    "# ç”Ÿæˆæ™‚ã®è¨­å®š\n",
    "generate_config = config[\"generate_config\"]\n",
    "# ãƒ‘ã‚¹è¨­å®š\n",
    "paths = config[\"paths\"]\n",
    "# ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®è¨­å®š\n",
    "dataset_config = config[\"dataset_config\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ã¨ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_config[\"model\"])\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_config[\"model\"], **model_config[\"model_kwargs\"]\n",
    ")\n",
    "\n",
    "# streamerã‚’ç”¨ã„ã‚‹ã¨ãƒ¢ãƒ‡ãƒ«ã®æ¨è«–æ™‚ã«æ¨™æº–å‡ºåŠ›ã«ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å‡ºåŠ›ã—ã¦ãã‚Œã¾ã™\n",
    "streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ãƒ¢ãƒ‡ãƒ«ã®æ¨è«–\n",
    "streamerã‚’ç”¨ã„ã‚‹ã“ã¨ã§ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å‡ºåŠ›ã—ã¦ãã‚Œã¾ã™ã€‚    \n",
    "ã¾ãŸã€é€šå¸¸ã®å‡ºåŠ›ã¯output[0]ã§ç¢ºèªã§ãã¾ã™ã€‚  \n",
    "å°ã•ãªãƒ¢ãƒ‡ãƒ«ãªã®ã§é•·æ–‡å‡ºåŠ›ã®ç²¾åº¦ã¯å¾®å¦™ã§ã™ã€‚  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "è¨€èªå‡¦ç†ã¯ã€äººé–“ã®è¨€èªã‚’å‡¦ç†ã™ã‚‹æŠ€è¡“ã§ã™ã€‚\n",
      "\n",
      "è¨€èªã®å‡¦ç†ã¯ã€å¤§ããåˆ†ã‘ã¦2ã¤ã‚ã‚Šã¾ã™ã€‚\n",
      "\n",
      "1ã¤ç›®ã¯ã€è¨€èªã®æ„å‘³ã‚’ç†è§£ã™ã‚‹å‡¦ç†ã§ã™ã€‚\n",
      "\n",
      "ä¾‹ãˆã°ã€ã€Œã‚Šã‚“ã”ã€ã¨ã„ã†è¨€è‘‰ã‚’èã„\n"
     ]
    }
   ],
   "source": [
    "text = \"è‡ªç„¶è¨€èªå‡¦ç†ã¨ã¯ä½•ã§ã™ã‹ï¼Ÿ\"\n",
    "tokenized_input = tokenizer.encode(text, add_special_tokens=False, return_tensors=\"pt\").to(model.device)\n",
    "with torch.no_grad():\n",
    "    output = model.generate(\n",
    "        tokenized_input,\n",
    "        **generate_config,\n",
    "        streamer=streamer\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "è‡ªç„¶è¨€èªå‡¦ç†ã¨ã¯ä½•ã§ã™ã‹ï¼Ÿ\n",
      "\n",
      "è¨€èªå‡¦ç†ã¯ã€äººé–“ã®è¨€èªã‚’å‡¦ç†ã™ã‚‹æŠ€è¡“ã§ã™ã€‚\n",
      "\n",
      "è¨€èªã®å‡¦ç†ã¯ã€å¤§ããåˆ†ã‘ã¦2ã¤ã‚ã‚Šã¾ã™ã€‚\n",
      "\n",
      "1ã¤ç›®ã¯ã€è¨€èªã®æ„å‘³ã‚’ç†è§£ã™ã‚‹å‡¦ç†ã§ã™ã€‚\n",
      "\n",
      "ä¾‹ãˆã°ã€ã€Œã‚Šã‚“ã”ã€ã¨ã„ã†è¨€è‘‰ã‚’èã„\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(output[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accelerator\n",
    "Acceleratorã¯è¤‡æ•°ã®GPUã‚’ç”¨ã„ã¦åˆ†æ•£å­¦ç¿’ã‚’è¡Œã†éš›ã«ä½¿ç”¨ã™ã‚‹ãƒ„ãƒ¼ãƒ«ã§ã™ã€‚  \n",
    "åˆ†æ•£å­¦ç¿’ã‚’è¡Œã‚ãªã„ã¨ãã«ã¯ä½¿ç”¨ã—ãªãã¦ã‚‚å•é¡Œã‚ã‚Šã¾ã›ã‚“ã€‚  \n",
    "å‚è€ƒï¼šhttps://qiita.com/m__k/items/518ac10399c6c8753763"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from accelerate import Accelerator\n",
    "\n",
    "# Acceleratorã®åˆæœŸåŒ–\n",
    "# accelerator = Accelerator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®æº–å‚™\n",
    "ã¾ãšã¯hugging faceã®datasetã‚’èª­ã¿è¾¼ã‚“ã§å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã¨æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã«åˆ†å‰²ã—ã¾ã™ã€‚  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä½œæˆ\n",
    "import datasets\n",
    "\n",
    "dolly_ja = datasets.load_dataset(dataset_config[\"name\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'output': 'ãƒ´ã‚¡ãƒ¼ã‚¸ãƒ³ãƒ»ã‚ªãƒ¼ã‚¹ãƒˆãƒ©ãƒªã‚¢èˆªç©ºã¯ã€2000å¹´8æœˆ31æ—¥ã«ãƒ´ã‚¡ãƒ¼ã‚¸ãƒ³ãƒ»ãƒ–ãƒ«ãƒ¼èˆªç©ºã¨ã—ã¦ã€2æ©Ÿã®èˆªç©ºæ©Ÿã§å˜ä¸€è·¯ç·šã®é‹èˆªã‚’é–‹å§‹ã—ã¾ã—ãŸã€‚',\n",
       " 'input': 'ãƒ´ã‚¡ãƒ¼ã‚¸ãƒ³ãƒ»ã‚ªãƒ¼ã‚¹ãƒˆãƒ©ãƒªã‚¢èˆªç©ºï¼ˆVirgin Australia Airlines Pty Ltdï¼‰ã¯ã‚ªãƒ¼ã‚¹ãƒˆãƒ©ãƒªã‚¢ã‚’æ‹ ç‚¹ã¨ã™ã‚‹ãƒ´ã‚¡ãƒ¼ã‚¸ãƒ³ãƒ»ãƒ–ãƒ©ãƒ³ãƒ‰ã‚’å† ã™ã‚‹æœ€å¤§ã®èˆ¹å›£è¦æ¨¡ã‚’æŒã¤èˆªç©ºä¼šç¤¾ã§ã™ã€‚2000å¹´8æœˆ31æ—¥ã«ã€ãƒ´ã‚¡ãƒ¼ã‚¸ãƒ³ãƒ»ãƒ–ãƒ«ãƒ¼ç©ºæ¸¯ã¨ã—ã¦ã€2æ©Ÿã®èˆªç©ºæ©Ÿã€1ã¤ã®ç©ºè·¯ã‚’é‹è¡Œã—ã¦ã‚µãƒ¼ãƒ“ã‚¹ã‚’é–‹å§‹ã—ã¾ã—ãŸã€‚2001å¹´9æœˆã®ã‚¢ãƒ³ã‚»ãƒƒãƒˆãƒ»ã‚ªãƒ¼ã‚¹ãƒˆãƒ©ãƒªã‚¢ç©ºæ¸¯ã®å´©å£Šå¾Œã€ã‚ªãƒ¼ã‚¹ãƒˆãƒ©ãƒªã‚¢ã®å›½å†…å¸‚å ´ã§æ€¥é€Ÿã«åœ°ä½ã‚’ç¢ºç«‹ã—ã¾ã—ãŸã€‚ãã®å¾Œã¯ãƒ–ãƒªã‚¹ãƒ™ãƒ³ã€ãƒ¡ãƒ«ãƒœãƒ«ãƒ³ã€ã‚·ãƒ‰ãƒ‹ãƒ¼ã‚’ãƒãƒ–ã¨ã—ã¦ã€ã‚ªãƒ¼ã‚¹ãƒˆãƒ©ãƒªã‚¢å›½å†…ã®32éƒ½å¸‚ã«ç›´æ¥ä¹—ã‚Šå…¥ã‚Œã‚‹ã¾ã§ã«æˆé•·ã—ã¾ã—ãŸã€‚',\n",
       " 'index': '0',\n",
       " 'category': 'closed_qa',\n",
       " 'instruction': 'ãƒ´ã‚¡ãƒ¼ã‚¸ãƒ³ãƒ»ã‚ªãƒ¼ã‚¹ãƒˆãƒ©ãƒªã‚¢èˆªç©ºã¯ã„ã¤ã‹ã‚‰é‹èˆªã‚’é–‹å§‹ã—ãŸã®ã§ã™ã‹ï¼Ÿ'}"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ä¸­èº«ã‚’ç¢ºèª\n",
    "dolly_ja['train'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ãƒˆãƒ¼ã‚¯ãƒ³åŒ–\n",
    "å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã‚’LLMã«å…¥åŠ›ã™ã‚‹ãŸã‚ã«ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ã‚’è¡Œã„ã¾ã™ã€‚  \n",
    "å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã¯ä»¥ä¸‹ã®ã‚ˆã†ãªãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã«ã—ã¦ã„ã¾ã™ãŒã€åˆ©ç”¨ã™ã‚‹ãƒ¢ãƒ‡ãƒ«ã«ã‚ˆã£ã¦é©åˆ‡ãªãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆãŒç•°ãªã‚‹å ´åˆãŒã‚ã‚‹ã“ã¨ã«æ³¨æ„ã—ã¦ãã ã•ã„ã€‚    \n",
    "```\n",
    "system: ã‚ãªãŸã¯å½¹ç«‹ã¤AIã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚ãƒ¦ãƒ¼ã‚¶ã‹ã‚‰ã®è³ªå•ã«å¯¾ã—ã¦å›ç­”ã‚’è¡Œã£ã¦ãã ã•ã„ã€‚\n",
    "\n",
    "user: è‡ªç„¶è¨€èªå‡¦ç†ã¨ã¯ä½•ã§ã™ã‹ï¼Ÿ\n",
    "\n",
    "assistant: è‡ªç„¶è¨€èªå‡¦ç†ã¯ã€ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ãŒäººé–“ã®è¨€èªã‚’ç†è§£ã—ã€ãã‚Œã‚’åˆ©ç”¨ã—ã¦ã‚³ãƒŸãƒ¥ãƒ‹ã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã‚’è¡Œã†ã“ã¨ã‚’å¯èƒ½ã«ã™ã‚‹æŠ€è¡“ã§ã™ã€‚\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13513/13513 [00:08<00:00, 1577.61 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1502/1502 [00:00<00:00, 1575.27 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®åˆ†å‰²\n",
    "train_valid_split = dolly_ja['train'].train_test_split(test_size=dataset_config[\"test_size\"])\n",
    "train_dataset = train_valid_split['train']\n",
    "valid_dataset = train_valid_split['test']\n",
    "\n",
    "# ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ãƒˆãƒ¼ã‚¯ãƒ³åŒ–é–¢æ•°\n",
    "def tokenize_function(example):\n",
    "    prompts = f\"system: ã‚ãªãŸã¯å„ªç§€ãªAIã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚ãƒ¦ãƒ¼ã‚¶ã‹ã‚‰ã®è³ªå•ã«å¯¾ã—ã¦ç°¡æ½”ã«å›ç­”ã‚’ã—ã¦ãã ã•ã„ã€‚\\n\\nuser: {example['instruction']}\\n\\ninput: {example['input']}\\n\\nassistant: {example['output']}\"\n",
    "    return tokenizer(prompts, padding=False, truncation=True, max_length=model_config[\"max_length\"])\n",
    "\n",
    "# ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ãƒˆãƒ¼ã‚¯ãƒ³åŒ–\n",
    "tokenized_train_dataset = train_dataset.map(tokenize_function)\n",
    "tokenized_valid_dataset = valid_dataset.map(tokenize_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'output': 'ã‚¢ãƒ•ãƒªã‚«ã€ã‚¢ã‚¸ã‚¢ã€ã‚¹ãƒšã‚¤ãƒ³ã€ãƒ™ãƒˆãƒŠãƒ ã€ä¸­å›½ã€ãƒ¨ãƒ¼ãƒ­ãƒƒãƒ‘ã€åŒ—ã‚¢ãƒ¡ãƒªã‚«',\n",
       " 'input': '',\n",
       " 'index': '5746',\n",
       " 'category': 'classification',\n",
       " 'instruction': 'ã“ã‚Œã‚‰ã¯å›½ãªã®ã‹å¤§é™¸ãªã®ã‹ã€æ•™ãˆã¦ãã ã•ã„',\n",
       " 'input_ids': [1,\n",
       "  1598,\n",
       "  28752,\n",
       "  39237,\n",
       "  29282,\n",
       "  69967,\n",
       "  11749,\n",
       "  58023,\n",
       "  78439,\n",
       "  64098,\n",
       "  78486,\n",
       "  76285,\n",
       "  99123,\n",
       "  68068,\n",
       "  29083,\n",
       "  75506,\n",
       "  79087,\n",
       "  18,\n",
       "  18,\n",
       "  1849,\n",
       "  28752,\n",
       "  39790,\n",
       "  29577,\n",
       "  29282,\n",
       "  29458,\n",
       "  56879,\n",
       "  68660,\n",
       "  79538,\n",
       "  71033,\n",
       "  79833,\n",
       "  18,\n",
       "  18,\n",
       "  2547,\n",
       "  28752,\n",
       "  279,\n",
       "  18,\n",
       "  18,\n",
       "  504,\n",
       "  11151,\n",
       "  28752,\n",
       "  41187,\n",
       "  29046,\n",
       "  58026,\n",
       "  29046,\n",
       "  60691,\n",
       "  29046,\n",
       "  63223,\n",
       "  29046,\n",
       "  65303,\n",
       "  29046,\n",
       "  64132,\n",
       "  29046,\n",
       "  29804,\n",
       "  58168],\n",
       " 'token_type_ids': [0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " 'attention_mask': [1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1]}"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_train_dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PEFTã‚’ç”¨ã„ãŸLoRAå­¦ç¿’ã®è¨­å®š\n",
    "Loraã®è¨­å®šã‚’è¡Œã„ã¾ã™ã€‚  \n",
    "å‚è€ƒï¼šhttps://qiita.com/t-hashiguchi/items/9f3b394ca0ae1c7e4d02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 10,080,256 || all params: 1,877,694,464 || trainable%: 0.5368\n"
     ]
    }
   ],
   "source": [
    "from peft import get_peft_model, LoraConfig, PeftModel\n",
    "\n",
    "model.enable_input_require_grads()\n",
    "model.gradient_checkpointing_enable(\n",
    "    gradient_checkpointing_kwargs={\"use_reentrant\": False}\n",
    ")\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    inference_mode=False,\n",
    "    target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\", \"embed_tokens\", \"lm_head\"],\n",
    "    r=model_config[\"lora_config\"][\"lora_r\"],\n",
    "    lora_alpha=model_config[\"lora_config\"][\"lora_alpha\"],\n",
    "    lora_dropout=model_config[\"lora_config\"][\"lora_dropout\"],\n",
    ")\n",
    "# ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ•ãƒªãƒ¼ã‚º\n",
    "for name, param in model.named_parameters():\n",
    "    if \"lora\" not in name and param.ndim == 1:\n",
    "        param.data = param.data.to(torch.bfloat16)\n",
    "\n",
    "# ãƒ¢ãƒ‡ãƒ«ã«LoRAã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼é©ç”¨ã€æ›´æ–°å¯¾è±¡ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ã®ç¢ºèª\n",
    "model = get_peft_model(model, peft_config)\n",
    "model.print_trainable_parameters()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### å­¦ç¿’ã®é–‹å§‹\n",
    "hugging faceã®trainerã‚’ä½¿ç”¨ã—ã¦å­¦ç¿’ã‚’è¡Œã„ã¾ã™ã€‚  \n",
    "training_argså†…ã®ç´°ã‹ã„è¨­å®šã¯LoRAã®webã‚µã‚¤ãƒˆã‚’å‚è€ƒã«ã—ã¦ãã ã•ã„ã€‚  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/saito/workspace/myenv/lib/python3.12/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ğŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='50' max='3379' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  50/3379 00:32 < 37:22, 1.48 it/s, Epoch 0.01/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[105], line 47\u001b[0m\n\u001b[1;32m     37\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m     38\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     39\u001b[0m     tokenizer\u001b[38;5;241m=\u001b[39mtokenizer,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     43\u001b[0m     data_collator\u001b[38;5;241m=\u001b[39mdata_collator,\n\u001b[1;32m     44\u001b[0m )\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# å­¦ç¿’ã‚’å®Ÿè¡Œ\u001b[39;00m\n\u001b[0;32m---> 47\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;66;03m# outputãƒ•ã‚©ãƒ«ãƒ€ã«å­¦ç¿’å¾Œã®ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜\u001b[39;00m\n\u001b[1;32m     50\u001b[0m model\u001b[38;5;241m.\u001b[39msave_pretrained(model_dir_path)\n",
      "File \u001b[0;32m~/workspace/myenv/lib/python3.12/site-packages/transformers/trainer.py:1938\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1936\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1937\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1938\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1939\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1940\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1941\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1942\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1943\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/workspace/myenv/lib/python3.12/site-packages/transformers/trainer.py:2279\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2276\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_begin(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[1;32m   2278\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[0;32m-> 2279\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2281\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2282\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2283\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m   2284\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   2285\u001b[0m ):\n\u001b[1;32m   2286\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2287\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m~/workspace/myenv/lib/python3.12/site-packages/transformers/trainer.py:3349\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   3347\u001b[0m         scaled_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m   3348\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3349\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3351\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mdetach() \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mgradient_accumulation_steps\n",
      "File \u001b[0;32m~/workspace/myenv/lib/python3.12/site-packages/accelerate/accelerator.py:2151\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   2149\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlomo_backward(loss, learning_rate)\n\u001b[1;32m   2150\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2151\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/workspace/myenv/lib/python3.12/site-packages/torch/_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    524\u001b[0m     )\n\u001b[0;32m--> 525\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/workspace/myenv/lib/python3.12/site-packages/torch/autograd/__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 267\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/workspace/myenv/lib/python3.12/site-packages/torch/autograd/graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    745\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from transformers import (\n",
    "    TrainingArguments,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    Trainer,\n",
    ")\n",
    "from datetime import datetime, timedelta, timezone\n",
    "\n",
    "# ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ä¿å­˜å…ˆã‚’è¨­å®š\n",
    "model_name = model_config[\"model\"].split(\"/\")[-1]\n",
    "JST = timezone(timedelta(hours=+9), \"JST\")\n",
    "dt_now = datetime.now(JST)\n",
    "now_time = dt_now.strftime(\"%Y%m%d_%H%M%S\")\n",
    "model_dir_path = f\"{paths[\"output_path\"]}/{model_name}/{now_time}\"\n",
    "\n",
    "# å­¦ç¿’æ™‚ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãªã©ã®è¨­å®š\n",
    "training_args = TrainingArguments(\n",
    "    per_device_train_batch_size=model_config['batch_size'],\n",
    "    per_device_eval_batch_size=model_config['batch_size'],\n",
    "    learning_rate=model_config['learning_rate'],\n",
    "    num_train_epochs=model_config['epochs'],\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=model_config['save_steps'],\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=model_config['logging_steps'],\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=model_config['logging_steps'],\n",
    "    output_dir=model_dir_path,\n",
    "    load_best_model_at_end=True,\n",
    "    greater_is_better=False,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    ")\n",
    "\n",
    "# å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã‚’ãƒãƒƒãƒå‡¦ç†ã™ã‚‹ãŸã‚ã®è¨­å®š\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
    "\n",
    "# Trainerã®åˆæœŸåŒ–\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=tokenized_train_dataset,\n",
    "    eval_dataset=tokenized_valid_dataset,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "# å­¦ç¿’ã‚’å®Ÿè¡Œ\n",
    "trainer.train()\n",
    "\n",
    "# outputãƒ•ã‚©ãƒ«ãƒ€ã«å­¦ç¿’å¾Œã®ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜\n",
    "model.save_pretrained(model_dir_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### å­¦ç¿’ã—ãŸãƒ¢ãƒ‡ãƒ«ã®ãƒ†ã‚¹ãƒˆ\n",
    "å­¦ç¿’ãŒå®Œäº†ã—ã¦ã„ãªã„ã®ã§ç¾çŠ¶ã®å‡ºåŠ›ã¯ã‚ã¾ã‚Šè‰¯ã„ã‚‚ã®ã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[å…¥åŠ›ãƒ‡ãƒ¼ã‚¿]\n",
      "instruction:\n",
      "RELXã¯ã©ã®ã‚ˆã†ãªæ ªä¾¡æŒ‡æ•°ã«å±ã—ã¦ã„ã‚‹ã®ã§ã™ã‹ï¼Ÿ\n",
      "input:\n",
      "RELX plcï¼ˆç™ºéŸ³ï¼šãƒ¬ãƒ«ã‚¨ãƒƒã‚¯ã‚¹ï¼‰ã¯ã€è‹±å›½ãƒ­ãƒ³ãƒ‰ãƒ³ã«æœ¬ç¤¾ã‚’ç½®ãè‹±å›½[2]ã®å¤šå›½ç±æƒ…å ±ãƒ»åˆ†æä¼æ¥­ã§ã™ã€‚ç§‘å­¦ãƒ»æŠ€è¡“ãƒ»åŒ»ç™‚æƒ…å ±ãŠã‚ˆã³åˆ†æã€æ³•å¾‹æƒ…å ±ãŠã‚ˆã³åˆ†æã€æ„æ€æ±ºå®šãƒ„ãƒ¼ãƒ«ã®æä¾›ã€å±•ç¤ºä¼šã®é–‹å‚¬ãªã©ã®äº‹æ¥­ã‚’å±•é–‹ã—ã¦ã„ã¾ã™ã€‚1993å¹´ã€ã‚¤ã‚®ãƒªã‚¹ã®æ›¸ç±ãƒ»é›‘èªŒå‡ºç‰ˆç¤¾ã§ã‚ã‚‹ãƒªãƒ¼ãƒ‰ãƒ»ã‚¤ãƒ³ã‚¿ãƒ¼ãƒŠã‚·ãƒ§ãƒŠãƒ«ã¨ã‚ªãƒ©ãƒ³ãƒ€ã®ç§‘å­¦å‡ºç‰ˆç¤¾ã§ã‚ã‚‹ã‚¨ãƒ«ã‚¼ãƒ“ã‚¢ã®åˆä½µã«ã‚ˆã‚Šèª•ç”Ÿã—ãŸä¼šç¤¾ã§ã™ã€‚\n",
      "\n",
      "åŒç¤¾ã¯ä¸Šå ´ä¼æ¥­ã§ã‚ã‚Šã€ãƒ­ãƒ³ãƒ‰ãƒ³è¨¼åˆ¸å–å¼•æ‰€ã€ã‚¢ãƒ ã‚¹ãƒ†ãƒ«ãƒ€ãƒ è¨¼åˆ¸å–å¼•æ‰€ã€ãƒ‹ãƒ¥ãƒ¼ãƒ¨ãƒ¼ã‚¯è¨¼åˆ¸å–å¼•æ‰€ã§æ ªå¼ã‚’å–å¼•ã—ã¦ã„ã¾ã™ï¼ˆãƒ†ã‚£ãƒƒã‚«ãƒ¼ã‚·ãƒ³ãƒœãƒ«ï¼šãƒ­ãƒ³ãƒ‰ãƒ³ï¼šRELã€ã‚¢ãƒ ã‚¹ãƒ†ãƒ«ãƒ€ãƒ ï¼šRENã€ãƒ‹ãƒ¥ãƒ¼ãƒ¨ãƒ¼ã‚¯ï¼šRELX).FTSE100æŒ‡æ•°ã€Financial Times Global 500ã€Euronext 100æŒ‡æ•°ã®æ§‹æˆéŠ˜æŸ„ã®ä¸€ã¤ã§ã™ã€‚\n",
      "output:\n",
      "RELX plcã¯ã€FTSE 100ã€Financial Times Global 500ã€Euronext 100ã®å„ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’æ§‹æˆã—ã¦ã„ã¾ã™ã€‚\n",
      "\n",
      "\n",
      "[å‡ºåŠ›]\n",
      "\n",
      "\n",
      "RELXã¯ã€ç§‘å­¦ãƒ»æŠ€è¡“ãŠã‚ˆã³åŒ»ç™‚æƒ…å ±ã€æ³•å¾‹æƒ…å ±ã€æ„æ€æ±ºå®šæ”¯æ´ãƒ„ãƒ¼ãƒ«ã®3ã¤ã®ä¸»è¦äº‹æ¥­åˆ†é‡ã‚’ã‚«ãƒãƒ¼ã™ã‚‹å¤šå›½ç±ä¼æ¥­ã§ã™ã€‚\n",
      "\n",
      "ç§‘å­¦ãƒ»æŠ€è¡“æƒ…å ±ã¯ã€ç§‘å­¦ãƒ»ãƒ†ã‚¯ãƒãƒ­ã‚¸ãƒ¼ãƒ»åŒ»å­¦ã®ç ”ç©¶ã¨é–‹ç™ºã«é–¢é€£\n"
     ]
    }
   ],
   "source": [
    "# ãƒ†ã‚¹ãƒˆç”¨ã®ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—\n",
    "test_data = tokenized_valid_dataset[0]\n",
    "\n",
    "print(f\"[å…¥åŠ›ãƒ‡ãƒ¼ã‚¿]\\ninstruction:\\n{test_data['instruction']}\\ninput:\\n{test_data['input']}\\noutput:\\n{test_data['output']}\\n\\n\")\n",
    "\n",
    "# ãƒ†ã‚¹ãƒˆç”¨ã®å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ã‚’æº–å‚™\n",
    "test_input_ids = torch.tensor(test_data['input_ids']).unsqueeze(0).to(model.device)\n",
    "\n",
    "# ãƒ¢ãƒ‡ãƒ«ã‚’è©•ä¾¡ãƒ¢ãƒ¼ãƒ‰ã«è¨­å®š\n",
    "model.eval()\n",
    "\n",
    "print(f\"[å‡ºåŠ›]\")\n",
    "# æ¨è«–ã‚’å®Ÿè¡Œ\n",
    "with torch.no_grad():\n",
    "    test_output = model.generate(test_input_ids, **generate_config,streamer=streamer)\n",
    "\n",
    "# çµæœã‚’ãƒ‡ã‚³ãƒ¼ãƒ‰ã—ã¦è¡¨ç¤º\n",
    "# print(\"å‡ºåŠ›:\", tokenizer.decode(test_output[0], skip_special_tokens=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
