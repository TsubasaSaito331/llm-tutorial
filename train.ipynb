{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 簡単なQ&Aを行うLLMを作成する\n",
    "このノートブックでは、Hugging FaceのTransformersとPEFTを使用して、簡単なQ&Aを行うLLMを作成します。  \n",
    "データセットはhugging faceで公開されているdatasetを使用してInstruction Tuningを行います。  \n",
    "今回使用するLLMは軽量な\"llm-jp/llm-jp-3-1.8b\"を使用します。  \n",
    "\n",
    "リンク：  \n",
    "- データセット：https://huggingface.co/datasets/izumi-lab/llm-japanese-dataset\n",
    "- モデル：https://huggingface.co/llm-jp/llm-jp-3-1.8b\n",
    "-  参考：https://qiita.com/m__k/items/173ade78990b7d6a4be4\n",
    "\n",
    "バージョン情報：  \n",
    "- python : 3.12.4\n",
    "- cuda : 12.1\n",
    "- transformers : 4.44.2\n",
    "- torch : 2.3.1+cu121\n",
    "- peft : 0.11.1\n",
    "- accelerate : 0.32.1\n",
    "- datasets : 2.20.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### モデルとトークナイザの読み込み\n",
    "config.yamlを読み込んで指定したモデルとトークナイザをダウンロードします。  \n",
    "初回実行時はダウンロードに時間がかかることがあります。  \n",
    "モデルはキャッシュとして保存されるため、必要のないモデルは削除してください。  \n",
    "モデルの保存先（デフォルト）：\n",
    "~/.cache/huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import yaml\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TextStreamer\n",
    "\n",
    "# YAMLファイルの読み込み\n",
    "with open(\"./config.yaml\", \"r\") as file:\n",
    "    config = yaml.safe_load(file)\n",
    "\n",
    "# モデルの設定\n",
    "model_config = config[\"model_config\"]\n",
    "# 生成時の設定\n",
    "generate_config = config[\"generate_config\"]\n",
    "# パス設定\n",
    "paths = config[\"paths\"]\n",
    "# データセットの設定\n",
    "dataset_config = config[\"dataset_config\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# トークナイザとモデルの読み込み\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_config[\"model\"])\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_config[\"model\"], **model_config[\"model_kwargs\"]\n",
    ")\n",
    "\n",
    "# streamerを用いるとモデルの推論時に標準出力にストリーミング出力してくれます\n",
    "streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### モデルの推論\n",
    "streamerを用いることでストリーミング出力してくれます。    \n",
    "また、通常の出力はoutput[0]で確認できます。  \n",
    "小さなモデルなので長文出力の精度は微妙です。  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "言語処理は、人間の言語を処理する技術です。\n",
      "\n",
      "言語の処理は、大きく分けて2つあります。\n",
      "\n",
      "1つ目は、言語の意味を理解する処理です。\n",
      "\n",
      "例えば、「りんご」という言葉を聞い\n"
     ]
    }
   ],
   "source": [
    "text = \"自然言語処理とは何ですか？\"\n",
    "tokenized_input = tokenizer.encode(text, add_special_tokens=False, return_tensors=\"pt\").to(model.device)\n",
    "with torch.no_grad():\n",
    "    output = model.generate(\n",
    "        tokenized_input,\n",
    "        **generate_config,\n",
    "        streamer=streamer\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "自然言語処理とは何ですか？\n",
      "\n",
      "言語処理は、人間の言語を処理する技術です。\n",
      "\n",
      "言語の処理は、大きく分けて2つあります。\n",
      "\n",
      "1つ目は、言語の意味を理解する処理です。\n",
      "\n",
      "例えば、「りんご」という言葉を聞い\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(output[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accelerator\n",
    "Acceleratorは複数のGPUを用いて分散学習を行う際に使用するツールです。  \n",
    "分散学習を行わないときには使用しなくても問題ありません。  \n",
    "参考：https://qiita.com/m__k/items/518ac10399c6c8753763"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from accelerate import Accelerator\n",
    "\n",
    "# Acceleratorの初期化\n",
    "# accelerator = Accelerator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### データセットの準備\n",
    "まずはhugging faceのdatasetを読み込んで学習データと検証データに分割します。  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データセットを作成\n",
    "import datasets\n",
    "\n",
    "dolly_ja = datasets.load_dataset(dataset_config[\"name\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'output': 'ヴァージン・オーストラリア航空は、2000年8月31日にヴァージン・ブルー航空として、2機の航空機で単一路線の運航を開始しました。',\n",
       " 'input': 'ヴァージン・オーストラリア航空（Virgin Australia Airlines Pty Ltd）はオーストラリアを拠点とするヴァージン・ブランドを冠する最大の船団規模を持つ航空会社です。2000年8月31日に、ヴァージン・ブルー空港として、2機の航空機、1つの空路を運行してサービスを開始しました。2001年9月のアンセット・オーストラリア空港の崩壊後、オーストラリアの国内市場で急速に地位を確立しました。その後はブリスベン、メルボルン、シドニーをハブとして、オーストラリア国内の32都市に直接乗り入れるまでに成長しました。',\n",
       " 'index': '0',\n",
       " 'category': 'closed_qa',\n",
       " 'instruction': 'ヴァージン・オーストラリア航空はいつから運航を開始したのですか？'}"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# データセットの中身を確認\n",
    "dolly_ja['train'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### トークン化\n",
    "学習データをLLMに入力するためにトークン化を行います。  \n",
    "学習データは以下のようなフォーマットにしていますが、利用するモデルによって適切なフォーマットが異なる場合があることに注意してください。    \n",
    "```\n",
    "system: あなたは役立つAIアシスタントです。ユーザからの質問に対して回答を行ってください。\n",
    "\n",
    "user: 自然言語処理とは何ですか？\n",
    "\n",
    "assistant: 自然言語処理は、コンピュータが人間の言語を理解し、それを利用してコミュニケーションを行うことを可能にする技術です。\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 13513/13513 [00:08<00:00, 1577.61 examples/s]\n",
      "Map: 100%|██████████| 1502/1502 [00:00<00:00, 1575.27 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# データセットの分割\n",
    "train_valid_split = dolly_ja['train'].train_test_split(test_size=dataset_config[\"test_size\"])\n",
    "train_dataset = train_valid_split['train']\n",
    "valid_dataset = train_valid_split['test']\n",
    "\n",
    "# データセットのトークン化関数\n",
    "def tokenize_function(example):\n",
    "    prompts = f\"system: あなたは優秀なAIアシスタントです。ユーザからの質問に対して簡潔に回答をしてください。\\n\\nuser: {example['instruction']}\\n\\ninput: {example['input']}\\n\\nassistant: {example['output']}\"\n",
    "    return tokenizer(prompts, padding=False, truncation=True, max_length=model_config[\"max_length\"])\n",
    "\n",
    "# データセットのトークン化\n",
    "tokenized_train_dataset = train_dataset.map(tokenize_function)\n",
    "tokenized_valid_dataset = valid_dataset.map(tokenize_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'output': 'アフリカ、アジア、スペイン、ベトナム、中国、ヨーロッパ、北アメリカ',\n",
       " 'input': '',\n",
       " 'index': '5746',\n",
       " 'category': 'classification',\n",
       " 'instruction': 'これらは国なのか大陸なのか、教えてください',\n",
       " 'input_ids': [1,\n",
       "  1598,\n",
       "  28752,\n",
       "  39237,\n",
       "  29282,\n",
       "  69967,\n",
       "  11749,\n",
       "  58023,\n",
       "  78439,\n",
       "  64098,\n",
       "  78486,\n",
       "  76285,\n",
       "  99123,\n",
       "  68068,\n",
       "  29083,\n",
       "  75506,\n",
       "  79087,\n",
       "  18,\n",
       "  18,\n",
       "  1849,\n",
       "  28752,\n",
       "  39790,\n",
       "  29577,\n",
       "  29282,\n",
       "  29458,\n",
       "  56879,\n",
       "  68660,\n",
       "  79538,\n",
       "  71033,\n",
       "  79833,\n",
       "  18,\n",
       "  18,\n",
       "  2547,\n",
       "  28752,\n",
       "  279,\n",
       "  18,\n",
       "  18,\n",
       "  504,\n",
       "  11151,\n",
       "  28752,\n",
       "  41187,\n",
       "  29046,\n",
       "  58026,\n",
       "  29046,\n",
       "  60691,\n",
       "  29046,\n",
       "  63223,\n",
       "  29046,\n",
       "  65303,\n",
       "  29046,\n",
       "  64132,\n",
       "  29046,\n",
       "  29804,\n",
       "  58168],\n",
       " 'token_type_ids': [0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " 'attention_mask': [1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1]}"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_train_dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PEFTを用いたLoRA学習の設定\n",
    "Loraの設定を行います。  \n",
    "参考：https://qiita.com/t-hashiguchi/items/9f3b394ca0ae1c7e4d02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 10,080,256 || all params: 1,877,694,464 || trainable%: 0.5368\n"
     ]
    }
   ],
   "source": [
    "from peft import get_peft_model, LoraConfig, PeftModel\n",
    "\n",
    "model.enable_input_require_grads()\n",
    "model.gradient_checkpointing_enable(\n",
    "    gradient_checkpointing_kwargs={\"use_reentrant\": False}\n",
    ")\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    inference_mode=False,\n",
    "    target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\", \"embed_tokens\", \"lm_head\"],\n",
    "    r=model_config[\"lora_config\"][\"lora_r\"],\n",
    "    lora_alpha=model_config[\"lora_config\"][\"lora_alpha\"],\n",
    "    lora_dropout=model_config[\"lora_config\"][\"lora_dropout\"],\n",
    ")\n",
    "# ベースモデルをフリーズ\n",
    "for name, param in model.named_parameters():\n",
    "    if \"lora\" not in name and param.ndim == 1:\n",
    "        param.data = param.data.to(torch.bfloat16)\n",
    "\n",
    "# モデルにLoRAアダプター適用、更新対象のパラメータ数の確認\n",
    "model = get_peft_model(model, peft_config)\n",
    "model.print_trainable_parameters()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 学習の開始\n",
    "hugging faceのtrainerを使用して学習を行います。  \n",
    "training_args内の細かい設定はLoRAのwebサイトを参考にしてください。  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/saito/workspace/myenv/lib/python3.12/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='50' max='3379' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  50/3379 00:32 < 37:22, 1.48 it/s, Epoch 0.01/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[105], line 47\u001b[0m\n\u001b[1;32m     37\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m     38\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     39\u001b[0m     tokenizer\u001b[38;5;241m=\u001b[39mtokenizer,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     43\u001b[0m     data_collator\u001b[38;5;241m=\u001b[39mdata_collator,\n\u001b[1;32m     44\u001b[0m )\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# 学習を実行\u001b[39;00m\n\u001b[0;32m---> 47\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;66;03m# outputフォルダに学習後のモデルを保存\u001b[39;00m\n\u001b[1;32m     50\u001b[0m model\u001b[38;5;241m.\u001b[39msave_pretrained(model_dir_path)\n",
      "File \u001b[0;32m~/workspace/myenv/lib/python3.12/site-packages/transformers/trainer.py:1938\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1936\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1937\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1938\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1939\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1940\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1941\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1942\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1943\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/workspace/myenv/lib/python3.12/site-packages/transformers/trainer.py:2279\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2276\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_begin(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[1;32m   2278\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[0;32m-> 2279\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2281\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2282\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2283\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m   2284\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   2285\u001b[0m ):\n\u001b[1;32m   2286\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2287\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m~/workspace/myenv/lib/python3.12/site-packages/transformers/trainer.py:3349\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   3347\u001b[0m         scaled_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m   3348\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3349\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3351\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mdetach() \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mgradient_accumulation_steps\n",
      "File \u001b[0;32m~/workspace/myenv/lib/python3.12/site-packages/accelerate/accelerator.py:2151\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   2149\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlomo_backward(loss, learning_rate)\n\u001b[1;32m   2150\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2151\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/workspace/myenv/lib/python3.12/site-packages/torch/_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    524\u001b[0m     )\n\u001b[0;32m--> 525\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/workspace/myenv/lib/python3.12/site-packages/torch/autograd/__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 267\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/workspace/myenv/lib/python3.12/site-packages/torch/autograd/graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    745\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from transformers import (\n",
    "    TrainingArguments,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    Trainer,\n",
    ")\n",
    "from datetime import datetime, timedelta, timezone\n",
    "\n",
    "# パラメータの保存先を設定\n",
    "model_name = model_config[\"model\"].split(\"/\")[-1]\n",
    "JST = timezone(timedelta(hours=+9), \"JST\")\n",
    "dt_now = datetime.now(JST)\n",
    "now_time = dt_now.strftime(\"%Y%m%d_%H%M%S\")\n",
    "model_dir_path = f\"{paths[\"output_path\"]}/{model_name}/{now_time}\"\n",
    "\n",
    "# 学習時のパラメータなどの設定\n",
    "training_args = TrainingArguments(\n",
    "    per_device_train_batch_size=model_config['batch_size'],\n",
    "    per_device_eval_batch_size=model_config['batch_size'],\n",
    "    learning_rate=model_config['learning_rate'],\n",
    "    num_train_epochs=model_config['epochs'],\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=model_config['save_steps'],\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=model_config['logging_steps'],\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=model_config['logging_steps'],\n",
    "    output_dir=model_dir_path,\n",
    "    load_best_model_at_end=True,\n",
    "    greater_is_better=False,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    ")\n",
    "\n",
    "# 学習データをバッチ処理するための設定\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
    "\n",
    "# Trainerの初期化\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=tokenized_train_dataset,\n",
    "    eval_dataset=tokenized_valid_dataset,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "# 学習を実行\n",
    "trainer.train()\n",
    "\n",
    "# outputフォルダに学習後のモデルを保存\n",
    "model.save_pretrained(model_dir_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 学習したモデルのテスト\n",
    "学習が完了していないので現状の出力はあまり良いものではありません。  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[入力データ]\n",
      "instruction:\n",
      "RELXはどのような株価指数に属しているのですか？\n",
      "input:\n",
      "RELX plc（発音：レルエックス）は、英国ロンドンに本社を置く英国[2]の多国籍情報・分析企業です。科学・技術・医療情報および分析、法律情報および分析、意思決定ツールの提供、展示会の開催などの事業を展開しています。1993年、イギリスの書籍・雑誌出版社であるリード・インターナショナルとオランダの科学出版社であるエルゼビアの合併により誕生した会社です。\n",
      "\n",
      "同社は上場企業であり、ロンドン証券取引所、アムステルダム証券取引所、ニューヨーク証券取引所で株式を取引しています（ティッカーシンボル：ロンドン：REL、アムステルダム：REN、ニューヨーク：RELX).FTSE100指数、Financial Times Global 500、Euronext 100指数の構成銘柄の一つです。\n",
      "output:\n",
      "RELX plcは、FTSE 100、Financial Times Global 500、Euronext 100の各インデックスを構成しています。\n",
      "\n",
      "\n",
      "[出力]\n",
      "\n",
      "\n",
      "RELXは、科学・技術および医療情報、法律情報、意思決定支援ツールの3つの主要事業分野をカバーする多国籍企業です。\n",
      "\n",
      "科学・技術情報は、科学・テクノロジー・医学の研究と開発に関連\n"
     ]
    }
   ],
   "source": [
    "# テスト用のデータを取得\n",
    "test_data = tokenized_valid_dataset[0]\n",
    "\n",
    "print(f\"[入力データ]\\ninstruction:\\n{test_data['instruction']}\\ninput:\\n{test_data['input']}\\noutput:\\n{test_data['output']}\\n\\n\")\n",
    "\n",
    "# テスト用の入力データを準備\n",
    "test_input_ids = torch.tensor(test_data['input_ids']).unsqueeze(0).to(model.device)\n",
    "\n",
    "# モデルを評価モードに設定\n",
    "model.eval()\n",
    "\n",
    "print(f\"[出力]\")\n",
    "# 推論を実行\n",
    "with torch.no_grad():\n",
    "    test_output = model.generate(test_input_ids, **generate_config,streamer=streamer)\n",
    "\n",
    "# 結果をデコードして表示\n",
    "# print(\"出力:\", tokenizer.decode(test_output[0], skip_special_tokens=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
